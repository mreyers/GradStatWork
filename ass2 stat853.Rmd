---
title: "Assignment 2 Stat853"
author: "Matthew Reyers, Dani Chu"
date: "February 11, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


1 Moore-Penrose pseudo inverse in reduced rank regression
1.1 Create a rank-deficient matrix. What happens?
```{r}
set.seed(1)
n = 100
x = runif(n)
y = 2*x + rnorm(n) #generate response vector y
x2 = x+100
X = cbind(1,x,x2)
beta.hat <- solve(t(X)%*%X,t(X)%*%y)

```
Running the above code on a singular matrix results in a failure to invert and causes the computation to fail. 

1.2 Singular value decomposition
Use R's svd() to obtain the full and thing decompositions.
```{r}
svd.mat <- svd(X, nu = nrow(X), nv = ncol(X))
svd.mat$d
```
For the full SVD, the svd object has the singular values stored in the d element. The corresponding singular values are `r svd.mat$d`. Note that the singular value for x2, the linearly dependent vector, is close to zero.
```{r}
# Thin svd is u_nxr, Sigma_rxr, V_rxp
# X_nxp, n = 100, p = 3, r= 2
d_r <- svd.mat$d[1:2] # Exclude third value
U_r <- svd.mat$u[1:100, 1:2] # Need r columns, r = 2
V_r <- t(svd.mat$v[1:3, 1:2]) # Need r columns and to store the result as a transpose
```
% Check this one %

1.3 Moore-Penrose pseudo inverse
Use the thin SVD to calculate X^-, the Moore-Penrose inverse of X. Compare coefficients with lm() results.
```{r}
sigma_r <- diag(x = d_r, nrow=2, ncol = 2)
mpInv <- t(V_r) %*% solve(sigma_r) %*% t(U_r) 

B_hat_inv <- mpInv %*% y
my_lm <- lm(y ~ x + x2)
# Need to manually set my_lm coefficient on 2 to 0 from NA
my_lm$coefficients[3] <- 0

# Check length of each solution
sum((y - X%*%B_hat_inv)^2)
sum((y - X%*%my_lm$coefficients)^2)

# Currently equal in length
```
Review the results of these lengths. Equivalence is fine but expectation is that the svd is shorter.


2 Constrained least squares
2.1 Construct a constraint matrix such that b1hat = b2hat and call it CC. 
```{r}
CC <- as.matrix(t(c(0, 1, -1)), nrow = 1) # 1 constraint so 1 row, 3 variables so 3 columns

```

2.2 Get the SVD of CC
```{r}
CC.svd <- svd(CC, nu = nrow(CC), nv = ncol(CC))

# Prove we can re-achieve CC from the svd
sigma_d <- t(c(CC.svd$d, 0, 0))
CC.again <- with(CC.svd, u %*% sigma_d %*% t(v))

# Establish V_bar(r)
V_bar_r <- CC.svd$v[, 2:3] #r = 1 in this case so V_bar_r is created by all columns from 2 to p, where  p = 3 in this case
```
After performing the svd and establishing the recombination to generate CC, we can take columns 2 and 3 from V as the span of the Null space. This is because the rank of CC is only 1, meaning that all columns beyond 1 are in the Null space.

2.3 Constrained least squares estimator
Follow the steps to generate the fit in the unconstrained and constrained settings.
```{r}
design <- X %*% V_bar_r

# Unconstrained
uncon_fit <- lm.fit(design, y)

# Constrained sltn: Premultiply by V_bar_r to the coeff estimates 
con_fit <- V_bar_r %*% uncon_fit$coefficients

# Earlier estimates
my_lm$coefficients

# Verify predictions
  # b_0lm = b_0_con + 100 b_2_con
my_lm$coefficients[1] - (con_fit[1] + 100*con_fit[3]) # 0 except for round off, satisfied
  #b_1lm = 2b_1_con
my_lm$coefficients[2] - 2 * con_fit[2] # 0 except for round off, satisfied
```
The estimates generated by our constrained least squares procedures satisfy our predictions. % Build on this explanation


3. Principal Components and Multidimensional Scaling

3.1 Simulate data for coordinates
```{r}
set.seed(1)
m1 = c(1,rep(0,9),rep(0,90))
m2 = c(0,1,rep(0,8),rep(0,90))
m3 = c(rep(0,8),1,0,rep(0,90))
m4 = c(rep(0,9),1,rep(0,90))
popms = rbind(m1,m2,m3,m4)

sigma_new <- 0.05
vcov_mat <- sigma_new^2 * diag(x = rep(1, n = 100), ncol = 100, nrow = 100)
library(MASS)

# Simulation of population, 2 steps
  nsim <- 1000
  # Uniform value to determine mvnorm parameters
  uni_samp <- runif(nsim) %>% data.frame() %>% mutate(group = case_when(
    . < 0.25 ~ "1",
    . >= 0.25 & . < 0.5 ~ "2",
    . >= 0.5 & . < 0.75 ~ "3",
    TRUE ~ "4"
  ))
  samp <- matrix( ncol = length(m1))
  for(i in 1:nrow(popms)){
    n_dist <- uni_samp %>% filter(group %in% as.character(i)) %>% summarize(n = n())
    reps <- mvrnorm(n = n_dist$n, mu = popms[i,], Sigma = vcov_mat)
    samp <- rbind(samp, reps)
  }
  samp <- samp[-1,]
```

 
3.2 Classical MDS 
```{r}
distances <- dist(samp)
scaled <- cmdscale(distances, k = 2)

# Big plot
names(scaled) <- c("x", "y")
scaled %>% data.frame(x = x, y = y) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("Classical MDS plotting") + theme_bw()
```

3.3 PCA with princomp
Do principle components down to 2 PC's using princomp. Plot the second component against the first.
```{r}
PCA <- princomp(samp)

# Plotting PC1 vs. PC2 should constitute plotting the scores against each other
temp <- PCA$scores[, 1:2]
names(temp) <- c("x", "y")
temp %>% data.frame(x = x, y = y) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("Principle Components on k = 2 using princomp") + theme_bw()
```
The generated plots from MDS and PCA on the same number of dimensions look nearly, if not, identical.


3.4 PCA with prcomp
Same as in 3.3 but using prcomp in place of princomp. Difference is in the fact that prcomp centers the data first.
```{r}
PCA2 <- prcomp(samp)

# Plotting PC1 vs. PC2 should constitute plotting the x values against each other
temp2 <- PCA2$x[, 1:2]
names(temp2) <- c("x", "y")
temp2 %>% data.frame(x = x, y = y) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("Principle Components on k = 2 using prcomp") + theme_bw()
```

3.5 PC Loadings
When the columns are centred, the principal components analysis of X^TX generates the variable loadings in the columns of V_r, the eigenvectors of X^TX.
```{r}
centred_X <- scale(samp, center = T, scale = F)
centred_svd <- svd(t(centred_X) %*% centred_X)
head(abs(centred_svd$v) - abs(PCA2$rotation))
```
The above is a manual calculation of the values of interest. Since an orthonormal basis is unique up to sign changes, a comparison of the absolute values of loadings is sufficient. All values fall within round off of zero, indicating we achieve the same loadings in both approaches.

3.6 Reflecting PCs
The plots I have generated for MDS, princomp, and prcomp are already aligned as the first two principle components happen to be the same throughout. My lazy approach would be to take the absolute value of each set of loadings (for MDS, princomp, and prcomp) and plot those. If MDS is to be left untouched, then I would take the loadings from MDS and add pairwise the corresponding loadings in each of the other methods. If a pairwise sum is 0 after round off to say 7 digits, then the components are negatives of one another. If the pairwise sum is not 0, then it is because the values are the same and we actually now have 2 * loading in that cell. I would then replace any vector that generated a zero with -1 * vector to get the desired orientation.

3.7 Computational speed of PCA approaches
a) Which function works on X^TX and which works on XX^T?
```{r}
f1=function(X){ 
  X=scale(X,center=TRUE,scale=FALSE)
  ss=eigen(t(X) %*% X)
  return(X%*%ss$vectors[,1:2]) 
  }

f2=function(X){ 
  X=scale(X,center=TRUE,scale=FALSE)
  ss=eigen(X %*% t(X))
  return(ss$vectors[,1:2]%*%diag(sqrt(ss$values[1:2])))
  }


```
f1 works on X^TX as it relies on being post multiplied by eigenvalues which are of the format p x 2 in this case as p < n.
f2 works on XX^T as it relies on multiplying 2 eigenvectors of dimension n by their corresponding eigenvalues in 2xn format as n < p.

b) When n > p, which of f1 and f2 is closest to princomp applied to X?
f1 is the function most similar to princomp when applied to X satisfying p < n. This is because f1 relies on X^TX which is most useful in the situation of p < n due to how the multiplication of the matrices is calculated.

c) Time the three approaches (f1, f2, prcomp) on two data sets
```{r}
# i)
system.time(f1(samp))
system.time(f2(samp))
system.time(prcomp(samp))

# ii)
system.time(f1(t(samp)))
system.time(f2(t(samp)))
system.time(prcomp(t(samp)))
```
Order in terms of elapsed time for i): f1, prcomp, f2
Order in terms of elapsed time for ii):f2, prcomp, f1
Since samp is 1000 x 100, f1 outperforms f2 and prcomp because it works with the smaller data structure immediately (100 x 100). f2 will work with the larger data structure while prcomp will have to first check whether n or p is larger. A similar argument can be made for the ordering in part ii).
