---
title: "Assignment 2 Stat853"
author: "Matthew Reyers, Dani Chu"
date: "February 11, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```


1 Moore-Penrose pseudo inverse in reduced rank regression
1.1 Create a rank-deficient matrix. What happens?
```{r}
set.seed(1)
n <- 100
x <- runif(n)
y <- 2*x + rnorm(n) #generate response vector y
x2 <- x+100
X <- cbind(1,x,x2)
#beta.hat <- solve(t(X)%*%X,t(X)%*%y)

```
Running the above code on a singular matrix results in a failure to invert and causes the computation to fail. We have commented out the problem line for Markdown ease.

1.2 Singular value decomposition
Use R's svd() to obtain the full and thin decompositions.
```{r}
svd.mat <- svd(X, nu = nrow(X), nv = ncol(X))
svd.mat$d
```
For the full SVD, the svd object has the singular values stored in the d element. The corresponding singular values are `r svd.mat$d`. Note that the singular value for $x_2$, the linearly dependent vector, is close to zero.
```{r}
# Thin svd is u_nxr, Sigma_rxr, V_rxp
# X_nxp, n = 100, p = 3, r= 2
d_r <- svd.mat$d[1:2] # Exclude third value
U_r <- svd.mat$u[1:100, 1:2] # Need r columns, r = 2
V_r <- t(svd.mat$v[1:3, 1:2]) # Need r columns and to store the result as a transpose

# Or, more simply
thin_svd <- svd(X, nu = 2, nv = 2)
```
The thin SVD is comprised of the components outlined above. Note the singular value that was close to 0, $x_2$, has been set exactly to 0 in this process.

1.3 Moore-Penrose pseudo inverse
Use the thin SVD to calculate $X^-$, the Moore-Penrose inverse of X. Compare coefficients with lm() results.
```{r}
sigma_r <- diag(x = d_r, nrow=2, ncol = 2)
mpInv <- t(V_r) %*% solve(sigma_r) %*% t(U_r) 

B_hat_inv <- mpInv %*% y
my_lm <- lm(y ~ x + x2)
# Need to manually set my_lm coefficient on 2 to 0 from NA
my_lm$coefficients[3] <- 0

# The fitted values for each approach
y_hat_svd <- X %*% B_hat_inv
y_hat_lm <- X %*% my_lm$coefficients

# Check length of each solution
l1 <- sqrt(sum((B_hat_inv)^2))
l2 <- sqrt(sum((my_lm$coefficients)^2))


```
The resulting lengths of the two approaches confirms what we expected. The length of the svd coefficients, `r round(l1, 4)`, is less than that of the lm coefficients, `r round(l2, 4)`. 


2 Constrained least squares
2.1 Construct a constraint matrix such that b1hat = b2hat and call it CC. 
```{r}
CC <- as.matrix(t(c(0, 1, -1)), nrow = 1) # 1 constraint so 1 row, 3 variables so 3 columns

```

2.2 Get the SVD of CC
```{r}
CC.svd <- svd(CC, nu = nrow(CC), nv = ncol(CC))

# Prove we can re-achieve CC from the svd
sigma_d <- t(c(CC.svd$d, 0, 0))
CC.again <- with(CC.svd, u %*% sigma_d %*% t(v))

# Establish V_bar(r)
V_bar_r <- CC.svd$v[, 2:3] #r = 1 in this case so V_bar_r is created by all columns from 2 to p, where  p = 3 in this case
```
After performing the svd and establishing the recombination to generate CC, we can take columns 2 and 3 from V as the span of the Null space. This is because the rank of CC is only 1, meaning that all columns beyond 1 are in the Null space.

2.3 Constrained least squares estimator
Follow the steps to generate the fit in the unconstrained and constrained settings.
```{r}
design <- X %*% V_bar_r

# Unconstrained
uncon_fit <- lm.fit(design, y)

# Constrained sltn: Premultiply by V_bar_r to the coeff estimates 
con_fit <- V_bar_r %*% uncon_fit$coefficients

# Earlier estimates
my_lm$coefficients

# Verify predictions
  # b_0lm = b_0_con + 100 b_2_con
(my_lm$coefficients[1] - (con_fit[1] + 100*con_fit[3]))[[1]] # 0 except for round off, satisfied
  #b_1lm = 2b_1_con
(my_lm$coefficients[2] - 2 * con_fit[2])[[1]] # 0 except for round off, satisfied
```
The estimates generated by our constrained least squares procedures satisfy our predictions as the differences in value are inaccurate only to a small round off error, meaning they are essentially identical.


3. Principal Components and Multidimensional Scaling

3.1 Simulate data for coordinates
```{r}
set.seed(1)
m1 = c(1,rep(0,9),rep(0,90))
m2 = c(0,1,rep(0,8),rep(0,90))
m3 = c(rep(0,8),1,0,rep(0,90))
m4 = c(rep(0,9),1,rep(0,90))
popms = rbind(m1,m2,m3,m4)

sigma_new <- 0.05
vcov_mat <- sigma_new^2 * diag(x = rep(1, n = 100), ncol = 100, nrow = 100)
library(MASS)

# Simulation of population, 2 steps
  nsim <- 1000
  # Uniform value to determine mvnorm parameters
  uni_samp <- runif(nsim) %>% data.frame() %>% mutate(group = case_when(
    . < 0.25 ~ "1",
    . >= 0.25 & . < 0.5 ~ "2",
    . >= 0.5 & . < 0.75 ~ "3",
    TRUE ~ "4"
  ))
  samp <- matrix( ncol = length(m1))
  for(i in 1:nrow(popms)){
    n_dist <- uni_samp %>% filter(group %in% as.character(i)) %>% summarize(n = n())
    reps <- mvrnorm(n = n_dist$n, mu = popms[i,], Sigma = vcov_mat)
    samp <- rbind(samp, reps)
  }
  samp <- samp[-1,]
```

 
3.2 Classical MDS 
```{r}
centred <- scale(samp, center = T, scale = F)
distances <- dist(centred)
scaled <- cmdscale(distances, k = 2)


# Big plot
scaled %>% data.frame(x = scaled[,1], y = scaled[,2]) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("Classical MDS plotting") +
  xlab("First Coordinate") + ylab("Second Coordinate") +
  theme_bw()
```

3.3 PCA with princomp
Do principle components down to 2 PC's using princomp. Plot the second component against the first.
```{r}
PCA_s <- princomp(centred)

# Plotting PC1 vs. PC2 should constitute plotting the scores against each other
PCARes <- PCA_s$scores[, 1:2]

PCARes %>% data.frame(x = PCARes[,1], y = PCARes[,2]) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("Principle Components on k = 2 using princomp") + xlab("First Coordinate") + ylab("Second Coordinate") + theme_bw()
```
The generated plots from MDS and PCA on the same number of dimensions look nearly identical, though one has an inverted second coordinate vlue.


3.4 PCA with prcomp
Same as in 3.3 but using prcomp in place of princomp. Difference is in the fact that prcomp centers the data first.
```{r}
PCA2 <- prcomp(centred)

# Plotting PC1 vs. PC2 should constitute plotting the x values against each other
PCA2Res <- PCA2$x[, 1:2]

PCA2Res %>% data.frame(x = PCA2Res[,1], y = PCA2Res[,2]) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("Principle Components on k = 2 using prcomp") + xlab("First Coordinate") + ylab("Second Coordinate") + theme_bw()
```
The PCA using prcomp is the same as MDS except with an inverted first coordinate value. 

3.5 PC Loadings
When the columns are centred, the principal components analysis of X^TX generates the variable loadings in the columns of V_r, the eigenvectors of X^TX.
```{r}
centred_X <- scale(samp, center = T, scale = F)
centred_svd <- svd(t(centred_X) %*% centred_X)
PCA <- princomp(samp)
head(abs(centred_svd$v) - abs(PCA$loadings))
```
The above is a manual calculation of the values of interest. Since an orthonormal basis is unique up to sign changes, a comparison of the absolute values of loadings is sufficient. All values fall within round off of zero, indicating we achieve the same loadings in both approaches.

3.6 Reflecting PCs
The plots we have generated for MDS, princomp, and prcomp are similar except for the sign. Given MDS as a base, we outlined in the previous questions what change would have to be made to make each identical. We suggested that the second PC be inverted for princomp while the first component should be inverted for prcomp. Suggested changes in their respective plots are below.
```{r}
# MDS
scaled %>% data.frame(x = scaled[,1], y = scaled[,2]) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("Classical MDS plotting") +
  xlab("First Coordinate") + ylab("Second Coordinate") +
  theme_bw()

# princomp
PCARes %>% data.frame(x = PCARes[,1], y = -PCARes[,2]) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("princomp To Look Like MDS") + xlab("First Coordinate") + ylab("Second Coordinate") + theme_bw()

# prcomp
PCA2Res %>% data.frame(x = -PCA2Res[,1], y = PCA2Res[,2]) %>% ggplot(aes(x = x, y = y)) + geom_point() + ggtitle("prcomp To Look Like MDS") + xlab("First Coordinate") + ylab("Second Coordinate") + theme_bw()
```


3.7 Computational speed of PCA approaches
a) Which function works on X^TX and which works on XX^T?
```{r}
f1=function(X){ 
  X=scale(X,center=TRUE,scale=FALSE)
  ss=eigen(t(X) %*% X)
  return(X%*%ss$vectors[,1:2]) 
  }

f2=function(X){ 
  X=scale(X,center=TRUE,scale=FALSE)
  ss=eigen(X %*% t(X))
  return(ss$vectors[,1:2]%*%diag(sqrt(ss$values[1:2])))
  }


```
f1 works on X^TX as it relies on being post multiplied by eigenvalues which are of the format p x 2 in this case as p < n.
f2 works on XX^T as it relies on multiplying 2 eigenvectors of dimension n by their corresponding eigenvalues in 2xn format as n < p.

b) When n > p, which of f1 and f2 is closest to princomp applied to X?
f1 is the function most similar to princomp when applied to X satisfying p < n. This is because f1 relies on X^TX which is most useful in the situation of p < n due to how the multiplication of the matrices is calculated.

c) Time the three approaches (f1, f2, prcomp) on two data sets
```{r}
# i)
system.time(f1(samp))
system.time(f2(samp))
system.time(prcomp(samp))

# ii)
system.time(f1(t(samp)))
system.time(f2(t(samp)))
system.time(prcomp(t(samp)))
```
Order in terms of elapsed time for i): f1, prcomp, f2
Order in terms of elapsed time for ii):f2, prcomp, f1
Since samp is 1000 x 100, f1 outperforms f2 and prcomp because it works with the smaller data structure immediately (100 x 100). f2 will work with the larger data structure while prcomp will have to first check whether n or p is larger. A similar argument can be made for the ordering in part ii).
