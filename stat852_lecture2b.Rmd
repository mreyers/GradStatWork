---
title: "Stat852 Assignment 1"
author: "Matthew Reyers"
date: "September 18, 2019"
output: html_document
---
# General
In interest of practicing some newer styles of coding, I have reworked most of the sample code. The general format is more closely adhering to Tidyverse code than it is to the base R version. As such, I will try to provide comments where I can to provide clarity.

# Lecture 2b)
The focus of this lecture has been on variable selection methods. Results are to be displayed and interpreted as per the corresponding questions.

## 1. a) Make a table of training and test error for the best model model built on eachhalf using each criterion in the program (sMSE, BIC, MSPE). Comment on the consistency of the results between the two halves: what is consistent and what is not. What does the consistency (or lack thereof) suggest about the relative importance of these variables.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(magrittr)
library(leaps)
library(knitr)
library(kableExtra)
library(conflicted)
conflict_prefer("extract", "magrittr")
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

prostate <-  read.table("Prostate.csv", header=TRUE, sep=",", na.strings=" ")
# head(prostate)

# Splitting data in half using random uniform selection to make two "set"s.
# Also performs the regression and stores the results / summary within the tibble for easy access
set.seed(120401002) 
prostate_tidy <- prostate %>%
  as.data.frame() %>%
  mutate(set = ifelse(runif(n = nrow(prostate)) > 0.5, "set1", "set2")) %>%
  nest(-set) %>%
  mutate(regsubs = map(data, ~ regsubsets(x = as.matrix(.x[, c(2:9)]), y = as.matrix(.x[, 10]),
                                          nbest = 1))) %>%
  mutate(summ_results = map(regsubs, ~ summary(.x)))

# Function I will need shortly
basic_reg <- function(data, n_vars, sizes){
  data <- data %>% select(-ID, -train)
  sizes <- sizes[, -1] # dont need intercept term manually listed
  if(n_vars == 0){
    return(lm(lpsa ~ 1, data = data))
  } else{
    return(lm(lpsa ~ ., data = data[, c(sizes[n_vars, ], TRUE)])) # Manually add in the response variable
  }
}

# Grabs the matrix of T/F values for regsubset models, the first one is for set1, the other set2
model_sizes_1 <- prostate_tidy %>% slice(1) %>% pull(summ_results) %>% extract2(1) %>% extract('which')
model_sizes_2 <- prostate_tidy %>% slice(2) %>% pull(summ_results) %>% extract2(1) %>% extract('which')

# Stuff for newdata later
set1 <- prostate_tidy %>%
  filter(set %in% "set1")
set2 <- prostate_tidy %>%
  filter(set %in% "set2")

# This combines the models with the necessary measurements (sMSE, BIC, MSPE) through extensive use of mapping
prostate_models_train1 <- prostate_tidy %>%
  filter(set %in% "set1") %>%
  mutate(n_vars = list(0:8)) %>% 
  unnest(.preserve = c(data, regsubs, summ_results)) %>%
  mutate(model_sizes = list(as.matrix(model_sizes_1$which)),
         reg_results = pmap(list(data, n_vars, model_sizes), ~ basic_reg(..1, ..2, ..3))) %>%
  mutate(sMSE = map_dbl(reg_results, ~ summary(.x)$sigma^2),
         BIC = pmap_dbl(list(reg_results, data), ~ extractAIC(..1, k = log(nrow(..2)))[2])) %>%
  mutate(new_data = set2$data,
         predictions = map2(reg_results, new_data, ~ predict(.x, newdata = .y)),
         MSPE = map2_dbl(predictions, new_data, ~ mean((.x - .y$lpsa)^2))) %>%
  select(n_vars, sMSE, BIC, MSPE, reg_results)

# Set up tibble to hold eventual results
results_firstseed <- tibble(TrainingSet = c(1,1,1,2,2,2),
                            Criterion = rep(c("sMSE", "BIC", "MSPE"), times = 2),
                            VarsChosen = as.character(rep(0, 6)),
                            TrainingError = rep(0, 6),
                            TestError = rep(0, 6))

sMSE_model_1 <- prostate_models_train1 %>%
  filter(sMSE == min(sMSE)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

BIC_model_1 <- prostate_models_train1 %>%
  filter(BIC == min(BIC)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

MSPE_model_1 <- prostate_models_train1 %>%
  filter(MSPE == min(MSPE)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

results_firstseed$VarsChosen[1] <- paste(sMSE_model_1 %>% names(), collapse = "+")
results_firstseed$VarsChosen[2] <- paste(BIC_model_1 %>% names(), collapse = "+")
results_firstseed$VarsChosen[3] <- paste(MSPE_model_1 %>% names(), collapse = "+")

results_firstseed$TrainingError[1] <- prostate_models_train1 %>%
  filter(sMSE == min(sMSE)) %>% pull(sMSE)
results_firstseed$TrainingError[2] <- prostate_models_train1 %>%
  filter(BIC == min(BIC)) %>% pull(sMSE)
results_firstseed$TrainingError[3] <- prostate_models_train1 %>%
  filter(MSPE == min(MSPE)) %>% pull(sMSE)

results_firstseed$TestError[1] <- prostate_models_train1 %>%
  filter(sMSE == min(sMSE)) %>% pull(MSPE)
results_firstseed$TestError[2] <- prostate_models_train1 %>%
  filter(BIC == min(BIC)) %>% pull(MSPE)
results_firstseed$TestError[3] <- prostate_models_train1 %>%
  filter(MSPE == min(MSPE)) %>% pull(MSPE)


# This does the same as the above but with respect to training on set2 and predicting on set1
prostate_models_train2 <- prostate_tidy %>%
  filter(set %in% "set2") %>%
  mutate(n_vars = list(0:8)) %>% 
  unnest(.preserve = c(data, regsubs, summ_results)) %>%
  mutate(model_sizes = list(as.matrix(model_sizes_2$which)),
         reg_results = pmap(list(data, n_vars, model_sizes), ~ basic_reg(..1, ..2, ..3))) %>%
  mutate(sMSE = map_dbl(reg_results, ~ summary(.x)$sigma^2),
         BIC = pmap_dbl(list(reg_results, data), ~ extractAIC(..1, k = log(nrow(..2)))[2])) %>%
  mutate(new_data = set1$data,
         predictions = map2(reg_results, new_data, ~ predict(.x, newdata = .y)),
         MSPE = map2_dbl(predictions, new_data, ~ mean((.x - .y$lpsa)^2))) %>%
  select(n_vars, sMSE, BIC, MSPE, reg_results)

sMSE_model_2 <- prostate_models_train2 %>%
  filter(sMSE == min(sMSE)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

BIC_model_2 <- prostate_models_train2 %>%
  filter(BIC == min(BIC)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

MSPE_model_2 <- prostate_models_train2 %>%
  filter(MSPE == min(MSPE)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

results_firstseed$VarsChosen[4] <- paste(sMSE_model_2 %>% names(), collapse = "+")
results_firstseed$VarsChosen[5] <- paste(BIC_model_2 %>% names(), collapse = "+")
results_firstseed$VarsChosen[6] <- paste(MSPE_model_2 %>% names(), collapse = "+")

results_firstseed$TrainingError[4] <- prostate_models_train2 %>%
  filter(sMSE == min(sMSE)) %>% pull(sMSE)
results_firstseed$TrainingError[5] <- prostate_models_train2 %>%
  filter(BIC == min(BIC)) %>% pull(sMSE)
results_firstseed$TrainingError[6] <- prostate_models_train2 %>%
  filter(MSPE == min(MSPE)) %>% pull(sMSE)

results_firstseed$TestError[4] <- prostate_models_train2 %>%
  filter(sMSE == min(sMSE)) %>% pull(MSPE)
results_firstseed$TestError[5] <- prostate_models_train2 %>%
  filter(BIC == min(BIC)) %>% pull(MSPE)
results_firstseed$TestError[6] <- prostate_models_train2 %>%
  filter(MSPE == min(MSPE)) %>% pull(MSPE)
# My understanding is that the base code has the groups reversed, meaning that my results will be in 
# the opposite order of what is expected (set1 for me I believe is labelled as 2 in original code)
```

The corresponding table for the first seed value is as follows:
```{r, message=FALSE}
results_firstseed %>% kable() %>% kable_styling()
```

The two halves display inconsistency with respect to the selected model, even within the same evaluation criterion. Considering this inconsistency, the few consistent features of the selected model bare a reasonable interpretation of being more important than the other variables. This would mean that `lcavol` and `svi` (featured 6 times), along with `lweight`  (featured 5 times) are likely the most important factors in an eventual model. 

## 1. b) Rerun the entire process with a new seed. Formulate a similar table as the above and comment on results.

Generation of the table for the second seed is an identical process. The table is displayed below but the code is suppressed.

```{r, echo=FALSE}
set.seed(9267926) 
prostate_tidy <- prostate %>%
  as.data.frame() %>%
  mutate(set = ifelse(runif(n = nrow(prostate)) > 0.5, "set1", "set2")) %>%
  nest(-set) %>%
  mutate(regsubs = map(data, ~ regsubsets(x = as.matrix(.x[, c(2:9)]), y = as.matrix(.x[, 10]),
                                          nbest = 1))) %>%
  mutate(summ_results = map(regsubs, ~ summary(.x)))

# Function I will need shortly
basic_reg <- function(data, n_vars, sizes){
  data <- data %>% select(-ID, -train)
  sizes <- sizes[, -1] # dont need intercept term manually listed
  if(n_vars == 0){
    return(lm(lpsa ~ 1, data = data))
  } else{
    return(lm(lpsa ~ ., data = data[, c(sizes[n_vars, ], TRUE)])) # Manually add in the response variable
  }
}

# Grabs the matrix of T/F values for regsubset models, the first one is for set1, the other set2
model_sizes_1 <- prostate_tidy %>% slice(1) %>% pull(summ_results) %>% extract2(1) %>% extract('which')
model_sizes_2 <- prostate_tidy %>% slice(2) %>% pull(summ_results) %>% extract2(1) %>% extract('which')

# Stuff for newdata later
set1 <- prostate_tidy %>%
  filter(set %in% "set1")
set2 <- prostate_tidy %>%
  filter(set %in% "set2")

# This combines the models with the necessary measurements (sMSE, BIC, MSPE) through extensive use of mapping
prostate_models_train1 <- prostate_tidy %>%
  filter(set %in% "set1") %>%
  mutate(n_vars = list(0:8)) %>% 
  unnest(.preserve = c(data, regsubs, summ_results)) %>%
  mutate(model_sizes = list(as.matrix(model_sizes_1$which)),
         reg_results = pmap(list(data, n_vars, model_sizes), ~ basic_reg(..1, ..2, ..3))) %>%
  mutate(sMSE = map_dbl(reg_results, ~ summary(.x)$sigma^2),
         BIC = pmap_dbl(list(reg_results, data), ~ extractAIC(..1, k = log(nrow(..2)))[2])) %>%
  mutate(new_data = set2$data,
         predictions = map2(reg_results, new_data, ~ predict(.x, newdata = .y)),
         MSPE = map2_dbl(predictions, new_data, ~ mean((.x - .y$lpsa)^2))) %>%
  select(n_vars, sMSE, BIC, MSPE, reg_results)

# This does the same as the above but with respect to training on set2 and predicting on set1
prostate_models_train2 <- prostate_tidy %>%
  filter(set %in% "set2") %>%
  mutate(n_vars = list(0:8)) %>% 
  unnest(.preserve = c(data, regsubs, summ_results)) %>%
  mutate(model_sizes = list(as.matrix(model_sizes_2$which)),
         reg_results = pmap(list(data, n_vars, model_sizes), ~ basic_reg(..1, ..2, ..3))) %>%
  mutate(sMSE = map_dbl(reg_results, ~ summary(.x)$sigma^2),
         BIC = pmap_dbl(list(reg_results, data), ~ extractAIC(..1, k = log(nrow(..2)))[2])) %>%
  mutate(new_data = set1$data,
         predictions = map2(reg_results, new_data, ~ predict(.x, newdata = .y)),
         MSPE = map2_dbl(predictions, new_data, ~ mean((.x - .y$lpsa)^2))) %>%
  select(n_vars, sMSE, BIC, MSPE, reg_results)

results_secondseed <- tibble(TrainingSet = c(1,1,1,2,2,2),
                            Criterion = rep(c("sMSE", "BIC", "MSPE"), times = 2),
                            VarsChosen = as.character(rep(0, 6)),
                            TrainingError = rep(0, 6),
                            TestError = rep(0, 6))

sMSE_model_1 <- prostate_models_train1 %>%
  filter(sMSE == min(sMSE)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

BIC_model_1 <- prostate_models_train1 %>%
  filter(BIC == min(BIC)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

MSPE_model_1 <- prostate_models_train1 %>%
  filter(MSPE == min(MSPE)) %>%
  pull(reg_results) %>%
  extract2(1) %>%
  summary() %>%
  use_series(coefficients) %>%
  extract(,1)

results_secondseed$VarsChosen[1] <- paste(sMSE_model_1 %>% names(), collapse = "+")
results_secondseed$VarsChosen[2] <- paste(BIC_model_1 %>% names(), collapse = "+")
results_secondseed$VarsChosen[3] <- paste(MSPE_model_1 %>% names(), collapse = "+")

results_secondseed$TrainingError[1] <- prostate_models_train1 %>%
  filter(sMSE == min(sMSE)) %>% pull(sMSE)
results_secondseed$TrainingError[2] <- prostate_models_train1 %>%
  filter(BIC == min(BIC)) %>% pull(sMSE)
results_secondseed$TrainingError[3] <- prostate_models_train1 %>%
  filter(MSPE == min(MSPE)) %>% pull(sMSE)

results_secondseed$TestError[1] <- prostate_models_train1 %>%
  filter(sMSE == min(sMSE)) %>% pull(MSPE)
results_secondseed$TestError[2] <- prostate_models_train1 %>%
  filter(BIC == min(BIC)) %>% pull(MSPE)
results_secondseed$TestError[3] <- prostate_models_train1 %>%
  filter(MSPE == min(MSPE)) %>% pull(MSPE)


results_secondseed$VarsChosen[4] <- paste(sMSE_model_2 %>% names(), collapse = "+")
results_secondseed$VarsChosen[5] <- paste(BIC_model_2 %>% names(), collapse = "+")
results_secondseed$VarsChosen[6] <- paste(MSPE_model_2 %>% names(), collapse = "+")

results_secondseed$TrainingError[4] <- prostate_models_train2 %>%
  filter(sMSE == min(sMSE)) %>% pull(sMSE)
results_secondseed$TrainingError[5] <- prostate_models_train2 %>%
  filter(BIC == min(BIC)) %>% pull(sMSE)
results_secondseed$TrainingError[6] <- prostate_models_train2 %>%
  filter(MSPE == min(MSPE)) %>% pull(sMSE)

results_secondseed$TestError[4] <- prostate_models_train2 %>%
  filter(sMSE == min(sMSE)) %>% pull(MSPE)
results_secondseed$TestError[5] <- prostate_models_train2 %>%
  filter(BIC == min(BIC)) %>% pull(MSPE)
results_secondseed$TestError[6] <- prostate_models_train2 %>%
  filter(MSPE == min(MSPE)) %>% pull(MSPE)
```

The corresponding table for the second seed value is as follows:

```{r, message=FALSE}
results_secondseed %>% kable() %>% kable_styling()
```

Using the second seed just to dictate the splitting of data, the chosen models again differ. Further, even the most frequently used variables change slightly with `lcavol` and `lweight` being included in every model while `svi` is only used in 5 of the models. 


## 1. c) From the two splits / 4 halves what impressions do I get about the ability of these procedures to identify the correct model and the relative importance of the variables in this particular data set.

The minimum values of each criterion differ between splits and between seeds. This suggests that there is no hard and fast decision to be made on the 'correct' model based on a criterion as there is inherent variability. Thoughts such as this give way to methods that could better deal with the randomness of splitting such as cross validation.

In the current context, considering all the splits and halves, I would be confident that the correct model would include `lcavol`, `lweight`, and `svi`. This confidence comes from the overwhelming presence of these variables throughout all 12 of the models fit above. 


## 2.a) Working with the abalone data, make a scatterplot and comment on relationship with Rings.

```{r}
# Abalone work, still assignment 1 of 852
library(tidyverse)
abalone <- read_csv("Abalone.csv", col_types = cols())

abalone <- abalone %>% 
  mutate(male_dummy = Sex == 1,
         female_dummy = Sex == 2) %>%
  select(-Sex) %>%
  select(Rings, everything())

abalone <- abalone %>%
  filter(Height > 0 & Height < 0.5)

pairs(~., abalone)
```

Using the above plot, there are a few variables such as `Length` and `Diameter` that should clear positive associations with `Rings`. The remaining variables display plots that could be noise or signal, though not to the same extent as `Length` and `Diameter`.

In terms of multicollinearity, I fully expect problems to arise. Both `Length` and `Diameter` are highly correlated as well as `Whole`, `Shucked`, `Viscera`, and `Shell`. An argument could be made that all of the measurements are correlated to the extent that they are problematic.

## 2.b) Split the data into 2 sets.
```{r}
set.seed(29003092)

abalone <- abalone %>%
  mutate(train = ifelse(runif(nrow(.)) > 0.75, "test", "train"))

abalone_train <- abalone %>% filter(train %in% "train") %>% dplyr::select(-train) %>% as.data.frame()
abalone_test <- abalone %>% filter(train %in% "test") %>% dplyr::select(-train) %>% as.data.frame()

```

## 2.c) Do forward stepwise and report the best model, according to BIC.
```{r, eval=FALSE}
library(leaps)

base_lm <- lm(Rings ~ 1, data = abalone_train)
full_lm <- lm(Rings ~ ., data = abalone_train)
summary(base_lm)

# c)
# k = log(n) for BIC calculation
forward_selec <- MASS::stepAIC(base_lm, direction = 'forward', 
                         scope = list(lower = base_lm, upper = full_lm), k = log(nrow(abalone_train)))

```

The best model using BIC is `Rings ~ Shell + Shucked + Height + Whole + Viscera + Diameter + male_dummy + female_dummy` with a BIC of `5043.66`. 

## 2.d) Do stepwise regression and report the best model, according to BIC. Does the model differ from forward stepwise?

```{r, eval=FALSE}
stepwise_selec <- MASS::stepAIC(full_lm, direction = 'both', 
                          scope = list(lower = base_lm, upper = full_lm), k = log(nrow(abalone_train)),
                          trace = TRUE)
```

The best model using BIC is `Rings ~ Shell + Shucked + Height + Whole + Viscera + Diameter + male_dummy + female_dummy` with a BIC of `5043.66`. The model selected is identical to forward stepwise on this data set split.

## 2.e) Do forward stepwise with no penalty term, manually calculating BIC at each step. Report the best model and discuss if previous runs stopped too early.

```{r}
forward_selec_no_pen <- MASS::stepAIC(base_lm, direction = 'forward', 
                         scope = list(lower = base_lm, upper = full_lm), k = 0)

log_like <- c(7424.8, 5876.02, 5423.15, 5215.6, 5141.72, 5086.4, 5033.45, 5009.7, 4971.12, 4970.62) %>%
  as.data.frame() %>%
  mutate(n_pred = row_number(),
         BIC = . + n_pred * log(nrow(abalone_train))
  )
```

The best model using BIC is `Rings ~ Shell + Shucked + Height + Whole + Viscera + Diameter + male_dummy + female_dummy` with a BIC of `5043.66`. The previous attempts have stopped at the same model, indicating that in this case they did not stop too early. 

## 2.f) Use all subsets regression. Report the best model. Look at each individual top model and discuss differences with forward selection path.

```{r}
regsubsets_res <- regsubsets(Rings ~ ., data = abalone_train, nbest = 1, nvmax = 10)
summary(regsubsets_res)$bic


```

According to the BIC calculated in all subsets regression, the best model is the complete model. In terms of arriving at this model, the all subsets regression path did not match the forward selection path. The differences between the paths become evident when we get to the model of 4 variables. The regsubsets approach allows Shell to exit the model while forward selection is greedy and cannot
backtrack on previous decisions. Naturally differences exist in future, larger models due to the same 
algorithmic discrepancy.

## 2.g) For each best model discovered, compare sample MSE and MSPE for the test set. How do these compare?

Assuming unique best models only is asking about the final model selected by each method, the following represents the desired comparison. Since all the stepwise methods arrived at the same model, they are refered to as `model_step` below. The all subsets regression generated a different model and is therefore represented as `model_regss`.

```{r}
model_regss <- lm(Rings ~ Shucked + Height + Whole + Viscera + Diameter +
                    male_dummy + female_dummy + Shell,
                  data = abalone_train)

model_step <- lm(Rings ~ Shell + Shucked + Height + Whole +
                   Viscera + Diameter + male_dummy + female_dummy + Length,
                 data = abalone_train) # Model forward is the same

# sMSE
summary(model_regss)$sigma^2
summary(model_step)$sigma^2

preds_regss <- predict(model_regss, newdata = abalone_test)
preds_step <- predict(model_step, newdata = abalone_test)

mean((preds_regss - abalone_test$Rings) ^ 2)
mean((preds_step - abalone_test$Rings) ^ 2)
```

Revisit tomorrow, I dont think the step model has Length.

The only difference between the models is the inclusion or exclusion of Length as a covariate. Expectedly, the results of sMSE and MSPE are similar. Despite being a small difference, the sMSE and MSPE are both smaller for the all subsets regression model.
