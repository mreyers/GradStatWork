---
title: "Assignment 1 Stat_851"
author: "Matthew Reyers and Dani Chu"
date: "January 10, 2019"
output: html_document
---
```{r}
pacman::p_load(tidyverse)
```


Stat 851 Assignment 1 writeup.

Question 1: Google does advertisements. In doing these ads, they charge companies dependent on some predefined approach. A common one is pay per click. Define the status of a click to be either yes (the clicker has made a purchase), undefined (the clicker has yet to reach the expiry time but has not yet made a purchase), and no (the clicker has not made a purchase within the given time). Represent these click values by (Y1, Y2, Y3) and probabilities (p1, p2, p3) respectively. 

a) Under which two conditions would this data be multinomially distributed? For each condition describe a way it could feasibly be broken.
For a vector (p1, p2, p3) to follow a multinomial distribution, assumptions similar to the binomial case need to be met. These assumptions are the independentness and the identicalness of each observation. Consider these in the scope of ad clicking. In a perfect (distribution) world, the clicks for each ad would be generated by a series of identically programmed robots, incapable of communicating with each other. In our actual world, each of these assumptions are lightly to heavily violated. It is clear that humans are not identical, the most obvious case being the difference in purchasing behaviour of males and females. Probabilities change based on what product is being sold and what sex is clicking the link, information that is not recorded. 
# Might be able to think of a better response for identicalness than this
Another issue is independence. Some purchases incentivize getting a friend involved. An example of this is in video game sales, where the discount is available to everyone but it is a cooperative game that requires 2+ people to play. This often results in me spending more money than I planned because someone bought a co-op game that looks like a lot of fun. So when I click the ad, I am not an independent replicate.


b) Compute the expected value of proportion of converted clicks and discuss its accuracy with respect to each violation listed.
Derivation of the expected value results in E(Y) = n * p, where Y is the random vector of observations for each category and p is the probability vector respectively. The derivation follows closely from the binomial expectation and is (hopefully) attached to the email for this submission. 
# Still need to do a write up on how violations alter expected value, if they do

c) Could Google control n and/or p1 such that the 95% Wald CI for p1 offers approximately 95% coverage?
If the Wald Confidence interval is to be approximately 95% coverage, Google would either need p1 to be close to 50% and/ or n to be large. I don't expect Google to have the capability to adjust p1 to that great of an extent so modifying n is the more reasonable route. This comes with a caveat, however, as the coverage will still likely zig-zag due to the discrete nature of our data.
# Review

d) Is p1hat unbiased for theta, the quantity of interest?

e) Is p1hat consistent for theta?


2. Let Y ~ bin(n, p). Compute the Wald, Aggresti-Coull, and Continuity corrected Wald CI for 100 different values of p, some of which are close to 0 or 1. 
```{r}
set.seed(12345)
p <- 0.01
n <- 100
phat <- 0:n / n
binprobs <- dbinom(0:n, n, p)

q <- qnorm(0.025)
Tn <- (phat - p) / sqrt(phat * (1 - phat) / n)
true_cov_Wald <- sum(binprobs[Tn <= - q]) - sum(binprobs[Tn <= q])

# Iterate above: Wald
p <- seq(0, 1, by = 0.01)#sample(seq(0.0001, 0.9999, by = 0.0001), size = 100)
true_cov_Wald <- data.frame(p = p, coverage = rep(0, n = length(p)))

overall_cov <- data.frame(p = p, Wald = rep(0, n = length(p)), AggCoul = rep(0, n = length(p)), WaldAdj = 
                          rep(0, n = length(p)))

for(i in 1:length(p)){
  Tn <- (phat - p[i]) / sqrt(phat * (1 - phat) / n)
  binprobs <- dbinom(0:n, n, p[i])
  true_cov_Wald[i, 2] <- sum(binprobs[Tn <= -q]) - sum(binprobs[Tn <= q])
}
overall_cov$Wald <- true_cov_Wald$coverage

# Sample plot
true_cov_Wald %>% arrange(p) %>% mutate(validity = case_when(coverage >= 0.95 ~ "Valid", TRUE ~ "Invalid")) %>% ggplot( aes(x = p, y = coverage, colour = validity)) + geom_point() + scale_y_continuous(limits = c(0.5, 1)) + geom_path(aes(group = 1)) 

# Ag-Coull: similar interval with p <- ptilde, n <- ntilde
ptilde <- (0:n + (q)^2 / 2) / (n + q^2)
ntilde <- n + q^2
true_cov_AC <- data.frame(p = p, coverage = rep(0, n = length(p)))
for(i in 1:length(p)){
  Tn_AC <- (ptilde - p[i]) / sqrt(ptilde * (1 - ptilde) / ntilde)
  binprobs <- dbinom(0:n, n, p[i])
  true_cov_AC[i, 2] <- sum(binprobs[Tn_AC <= -q]) - sum(binprobs[Tn_AC <= q])
}
overall_cov$AggCoul <- true_cov_AC$coverage

true_cov_AC %>% arrange(p) %>% mutate(validity = case_when(coverage >= 0.95 ~ "Valid", TRUE ~ "Invalid")) %>% ggplot( aes(x = p, y = coverage, colour = validity)) + geom_point() + scale_y_continuous(limits = c(0.5, 1)) + geom_path(aes(group = 1)) 

# Wald Adj.
true_cov_Wald_adj <- data.frame(p = p, coverage = rep(0, n = length(p)))
for(i in 1:length(p)){
  Tn <- (phat - p[i]) / sqrt(phat * (1 - phat) / n)
  binprobs <- dbinom(0:n, n, p[i])
  true_cov_Wald_adj[i, 2] <- sum(binprobs[Tn <= -q + 1/ (2*n)]) - sum(binprobs[Tn <= q - 1 / (2 * n)])
}
overall_cov$WaldAdj <- true_cov_Wald_adj$coverage

true_cov_Wald_adj %>% arrange(p) %>% mutate(validity = case_when(coverage >= 0.95 ~ "Valid", TRUE ~ "Invalid")) %>% ggplot( aes(x = p, y = coverage, colour = validity)) + geom_point() + scale_y_continuous(limits = c(0.5, 1)) + geom_path(aes(group = 1)) 

# Altogether plot
overall_cov %>% ggplot(aes(x = p)) + 
  geom_line(aes( y = Wald, colour = "Wald", group = 1)) + geom_line(aes(y = AggCoul, colour = "AgCoul", group = 1)) +
  geom_line(aes(y = WaldAdj, colour = "WaldAdj", group = 1)) + 
  geom_hline(aes(yintercept = 0.95, colour = "95%")) +
  ggtitle("True Coverage probabilities for 95% intervals") + labs(x = "Probability", y = "Coverage")
```
The plot above demonstrates the true coverage detected at multiple values of p for the three different interval types. We find from this plot that the Agresti-Coull interval offers a valid interval over the most probability values, a desirable property for many types of inference. The tradeoff is of course longer interval lengths for the Agresti-Coull than the other two intervals. A final note is that the Wald Adjustment makes only minimal performance gains over the Wald interval and fails to address the problem with the probabilities near 0, 1. As such, any decisions for/against using the Wald interval are applicable to the Wald Adjusted interval as well.


