---
title: "Stat850 Assignment 4"
author: "Matthew Reyers"
date: "November 8, 2018"
output: html_document
---

This report contains my code based submission for Assignment 4 of Stat 850. 

Question 1: Consider the following model that I used in assignment 1:
y = 10sin(4pi^0.9)exp(-1.5x) + 10 + epsilon, where epsilon follows a N(0, 4) distribution. I assume the 4 to be representative of the variance, meaning that the standard deviation is 2.

This question will fit a GP to the 11 observations as in the assignment 1. Use a Gaussian process of the form given in the assignment question.
```{r}
library(tidyverse)

# Step 0: set up as before
set.seed(1234)
x <- seq(0, 1, by = 0.1)
e <- rnorm(n = 11, sd = 2)

y <- 10*sin(4*pi*x^0.9)*exp(-1.5*x) + 10 + e
```

a) Write a function or subroutine to compute the covariance between two observations.
```{r}

covCalcGaussian <- function(x1, x2, theta){
  
  covPW <- exp(-theta * (x1 - x2) ^ 2) # 1 dimensional predictors, given theta
  return(covPW)
}
```

b) Write a function or subroutine to compute the covariance matrix for the vector of 11 observations
```{r}

covMatrixR <- function(X1, X2, theta){
  
  # Create the matrix
  nrow <- length(X1)
  ncol <- length(X2)
  if(nrow != ncol){
    print("Not covariance function eligible")
    return(NA)
  }
  covMatrix <- matrix(0, nrow, ncol)
  
  # Fill the matrix with values
  for(i in 1:nrow){
    for(j in 1:ncol){
      covMatrix[i, j] <- covCalcGaussian(X1[i], X2[j], theta)
    }
  }
  return(covMatrix)
}

covMatrixGaussian <- function(sigmaz, sigmae, covMatrix){
  sysCov <- (sigmaz)^2 * covMatrix + (sigmae)^2 * diag(dim(covMatrix)[1]) # Covariance of system depends on noise and signal
  return(sysCov)  
}

# Work in pairs, one feeds into the other
covMatrixtemp <- covMatrixR(x, x, 5)
covMatrixG <- covMatrixGaussian(1, 1, covMatrixtemp)

```

c) Compute the inverse of the covariance function
```{r}
invCov <- solve(covMatrixG)

# Some test values
testCovMatrix <- covMatrixR(x, x, 1000) # 
testCovGaussian <- covMatrixGaussian(1, 1, testCovMatrix)
```
I have not encountered any problems with tests on theta between 0 and 1000 inclusive.

d) Write a function or subroutine to compute the log of the Gaussian likelihood for a Gaussian process
```{r }
nllFunction <- function(mu, sigmaz, sigmae, theta ){
  # Function calculates likelihood value given parameter values
  
  # Covariance structure
  
covMatrixtemp <- covMatrixR(x, x, theta)
covMatrixG <- covMatrixGaussian(sigmaz, sigmae, covMatrixtemp)
  
  # Log likelihood requirements
  invSigma <- solve(covMatrixG) 
  muvec <- rep(mu, n = length(mu))
  exponent <- -1*t(y - muvec) %*% invSigma %*% (y - muvec) / 2 # Relies on the actual y defined prior to step a)

  # Likelihood and negative log likelihood
  loglike <- -1*log(det(2*pi*covMatrixG)) / 2 + exponent # Not negative LL, just the decomp of Gaussian Likelihood
  nll <- -1*loglike # Negative LL of the Gaussian
  
  return(nll)
}

# Example
nllFunction(mu = mean(y), theta = 5, sigmaz = 2, sigmae = 2)


```

e) Maximize the log likelihood function
We expect the mle for the mean of a Gaussian to be the mean of the sample and as such we will initialize the MLE conditions with mu = mean(y). Preliminary exploration supported this conclusion as all MLE results with the mean an adjustable parameter yielded the mean +/- some small error. The MLE used in the first component following appears to be getting trapped in local optima. Evidence of such is that switching the MLE estimates for sigmaz and sigmae resulted in a lower nLL, indicating this to be a better fit. I instead employ a grid based design using the values in the neighbourhoods found to be relevant by previous MLE work.
```{r}
library(stats4)
# MLE calculation is for all 4 of the parameters
mleResults <- mle(nllFunction, start = list(mu = mean(y), theta = 10, sigmaz = 1, sigmae = 1))
estimates <- mleResults@coef
# This mle seems to just accept theta at its given value (roughly)

# Note: MLE function seems to be wonky in this case
# If I switch the sigmaz and sigmae estimates in the nll function, I generate a smaller value
# E.g.
nllFunction(mu = mleResults@coef[1], theta = mleResults@coef[4], sigmaz = mleResults@coef[2], sigmae = mleResults@coef[3]) # 68.48561

nllFunction(mu = mleResults@coef[1], theta = mleResults@coef[4], sigmaz = mleResults@coef[3], sigmae = mleResults@coef[2]) # 57.09997

# This is unexpected behaviour. Instead I will use a disgusting series of nested for loops to investigate nll
  # This will be done with fixed muhat = mean(y) as this is the only consistent result
mu <- c()
theta <- c()
sigmaz <- c()
sigmae <- c()
nll <- c()

muhat <- mean(y)

parameters <- data.frame(mu, theta, sigmaz, sigmae, nll)

# Ran this for loop: Found my minimum on theta = 79, sigmaz = 5, sigmae = 1
  # nllResults = 32.63072
# 
# # # Loops over some chosen values for the parameters
# for(i in 1:100){
#   # Theta loop
#   for(j in 1:50){
#     # Sigmaz loop
#     for(k in 1:50){
#       # Sigmae loop
#       nllResults <- nllFunction(mu = muhat, sigmaz = j, sigmae = k, theta = i)[[1]]
#       rowResults <- cbind(muhat, i, j, k, nllResults)
#       parameters <- rbind(parameters, rowResults)
#     }
#   }
# }

# Switched to manual assign to save run time during Knit
# iterEst <- parameters[parameters$nllResults == min(parameters$nllResults), 1:4]
iterEst <- data.frame(mean(y), 79, 5, 1)
names(iterEst) <- c("mu", "theta", "sigmaz", "sigmae")



```



f) Using a random sample 50 inputs between 0 and 1, predict the output at the 50 different inputs
```{r}
set.seed(2)
xstar <- runif(50)

# Write new covariance matrix setup as prediction requires multiplication by sigmaz after choleski
predCovPointwise <- function(xstar_1, fullX, thetaHat){
  # Gets pointwise covariance of a new input value against the original dataset of inputs given a theta
  
  covVector <- exp(-thetaHat * (xstar_1 - fullX)^2)
}

predCovMatrix <- function(xstar_vec, fullX, thetaHat){
  # Generates the covariance matrix for prediction iterating inputs through the helper function predCovPointwise
  
  nrow <- length(xstar_vec)
  ncol <- length(fullX)
  covMatrix <- matrix(0, nrow, ncol)
  
  # Fill the matrix with values
  for(i in 1:nrow){
    covMatrix[i, ] <- predCovPointwise(xstar_vec[i], fullX, thetaHat)
  }
  return(covMatrix)
}

# Switching the following

mleEst <- iterEst
covMatrixPred <- predCovMatrix(xstar, x, mleEst$theta)
muhatVec <- rep(mleEst$mu, n = length(x))

sigmaHatTemp <- covMatrixR(x, x, mleEst$theta)
sigmaHat <- covMatrixGaussian(mleEst$sigmaz, mleEst$sigmae, sigmaHatTemp)

# Basic prediction
x1star <- xstar[1]
yhatx1star <- mleEst$mu + mleEst$sigmaz^2 * t(predCovPointwise(x1star, x, mleEst$theta))  %*% solve(sigmaHat) %*% (y - muhatVec)

# All yhats, iterate
yhat <- c()
for(i in 1:length(xstar)){
  yhatTemp <- mleEst$mu + mleEst$sigmaz^2 * t(predCovPointwise(xstar[i], x, mleEst$theta))  %*% solve(sigmaHat) %*% (y - muhatVec)
  yhat <- rbind(yhat, yhatTemp)
}

actualData <- data.frame(x, y)
predData <- data.frame(xstar, yhat)
ggplot(data = predData, aes(x = xstar, y = yhat)) + geom_point() + geom_line() + ggtitle("Predicted values (Black) against Actual Values (Red)") + geom_point(data = actualData, aes(x = x, y = y, col = "red"))  + geom_line(data = actualData, aes(x = x, y = y, col = "red"))

```


g) Compute (i) the maximum error over your 50 different inputs and (ii) the mean squared prediction error over your 50 different inputs
```{r}
# Maximum error
yTot <- 10*sin(4*pi*xstar^0.9)*exp(-1.5*xstar) + 10
error <- yhat - yTot

```
The maximum error achieved in predictions against the model (with no error term) was `r max(error)`. The mean squared prediction error over these same 50 observations was `r mean((yhat - yTot)^2)`

h) How does this compare to the polynomial regression from assignment 1?
In assignment 1, my analysis suggested that I use a 6th degree polynomial. This was more than likely an overfit and resulted in a MSPE of only 1.3326. In comparison, the model here generates an MSPE of `r mean((yhat - yTot)^2)`. If I had instead used a lower degree polynomial (such as degree 1 or 2) in assignment 1, then this Gaussian fit would likely demonstrate a large degree of improvement due to its flexibility.

2. A study was conducted to see if the height of a nest would impact the weights of baby birds. Nests were placed at different heights and the weights of baby birds one week after hatching were measured. The data are in the code below
```{r}
# Bird work
Nest <- c(1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 5)
Height <- c(10, 10, 10, 10, 15, 15, 15, 15, 20, 20, 20)
Weight <- c(7.5, 8.0, 7.1, 6.7, 9.1, 8.6, 9.6, 9.2, 9.9, 9.8, 10.2)
birds <- data.frame(Nest, Height, Weight)
```
a) What model would you fit to these data?
This situation involves an outcome (weight) and factors believed to possibly impact the outcome. The Nest factor represents a random factor as it is a variable we did not control for nor have a research question pertaining to. The Height factor is instead a fixed effect factor as we specifically changed the height of the nests and are looking to draw conclusions about these heights and their relation to one another. The setup is suggestive of a Mixed Effects Model and is what I will implement to better understand the data.

b) Fit the model and report your conclusions with respect to the scientific question of interest
```{r}
# Plot the data
birds %>% ggplot(aes(factor(Height), Weight)) + 
  geom_point() +
  ggtitle("Plot of bird weight against nest height") +
  theme_minimal()

```
Initial data plotting does not seem to indicate any excessive outliers or cause for concern about equal variance among groups. With no assumptions horribly violated, I will proceed with fitting my Mixed Effect Model.
```{r}
library(lme4)
birds <- birds %>% mutate(Height = factor(Height),
                 Nest = factor(Nest))
linMixModel <- lmer(Weight ~ Height + (1 | Nest), data = birds, REML = FALSE)
summary(linMixModel)
plot(fitted(linMixModel), residuals(linMixModel), xlab = "Fitted values", ylab = "Residuals")

# Out of curiosity, compare Mixed effects model to linear regression
standardLM <- lm(Weight ~ Height + Nest, data = birds)
summary(standardLM)
BIC(standardLM)
AIC(standardLM)
```
The Mixed Effect Model fit above suggests that the Fixed Effect Height and a non-zero intercept term constitute a useful model for understanding the weight of young birds. I would conclude that the Height of the nest has a positive effect on the weight of birds during development, meaning that birds that develop in nests further from the ground tend to be heavier. Further, the fit indicated a small amount of the variation in Weight is attributable to the Random Effect Nest. This amount is nearly identical to the amount explained by noise and suggests that the random effect may not be necessary for the model.

Comparison with the linear regression equivalent (done by treating Nest as a regressor but not a random effect) yields a model that possesses similar AIC and BIC scores. The anova function is unable to compare across model types, such as Mixed Effect vs. Linear Regression, so our comparison toolkit is somewhat limited. As such I compared AIC and BIC scores of each model. The Random Intercept Model is sufficient in comparison for my purposes.


3. Ventilation in tests to exhaustion. Model is roughly W = aV^2 + bV^3 + epsilon. Differences exist on an individual level as a and b are population parameters. May also be effect of sex, impacting slope of each.

a) Propose a model relating work of breathing to ventilation that accounts for the multiple 
measurements per person and also sex differences.

It was suggested that each person differs from the population results by some quantity. Since we have multiple observations for each person, it would be of use for us to calculate their individual values according to some model. I would suggest identifying an individual's specific a and b values according to a regression equation fit to their observations specifically. This would then lead to a population level random effects model in which W = (a + alpha_i)V^2 + (b + beta_i)V^3 + epsilon. A consideration to make is that each person's individual deviation from the population may be the cause of multiple differences. Some of these differences are expressable in fixed effects (sex) and random effects (individual performance differences). Model design should aim to incorporate both.

This suggestion is carried out in the lmer() function call below.

b) Fit the model.
```{r}
# Model
wob <- read.table("work-of-breathing.txt", header = T)
str(wob)

# Preliminary check on Sex having an effect on Work of Breathing
wob %>% lm(WOB ~ Sex, .) %>% summary()

# Use of this function from: https://www.r-bloggers.com/random-regression-coefficients-using-lme4/
res <- wob %>% lmer(WOB ~ -1 + Ve2 + Ve3 + (Ve2 + Ve3):Sex + (0 + Ve2| Subject) + (0 + Ve3| Subject), data = .)
summary(res)
```
The model above is designed to account for what we wanted to capture in the statement. It has no intercept term as I removed it from the process (-1) and have also tied up variables that would be non-zero if V = 0 in interactions with those terms that zero out. Sex is included in the model as there appears to be an effect of sex on the work of breathing, denoted by the t-value on its components in the summary and in a previous modeling simply including sex. Further, note that the random effect of subject is captured for both Ve2 and Ve3 without an intercept component (thus the (0 + Ve2 or Ve3)). 

c) Using an information criterion to select the “best” model for the data (collection of terms 
in the statistical model)
```{r}
AIC(res)

# Test models
resNoSubVe3 <- wob %>% lmer(WOB ~ -1 + Ve2 + Ve3 + (Ve2 + Ve3):Sex + (0 + Ve2| Subject), data = .) 
AIC(resNoSubVe3)

resNoSubVe2 <- wob %>% lmer(WOB ~ -1 + Ve2 + Ve3 + (Ve2 + Ve3):Sex  + (0 + Ve3| Subject), data = .)
AIC(resNoSubVe2)

resNoSubVe2Ve3 <- wob %>% lm(WOB ~ -1 + Ve2 + Ve3 + (Ve2 + Ve3):Sex, data = .)
AIC(resNoSubVe2Ve3)

resNoSex <- wob %>% lmer(WOB ~ -1 + Ve2 + Ve3 + (0 + Ve2| Subject) + (0 + Ve3| Subject), data = .)
AIC(resNoSex)

resNoVe2 <- wob %>% lmer(WOB ~ -1  + Ve3 + ( Ve3):Sex + (0 + Ve3| Subject), data = .)
AIC(resNoVe2)

resNoVe3 <- wob %>% lmer(WOB ~ -1 + Ve2  + (Ve2 ):Sex + (0 + Ve2| Subject), data = .)
AIC(resNoVe3)

resIntercept <- wob %>% lmer(WOB ~ Ve2 + Ve3 + (Ve2 + Ve3):Sex + (0 + Ve2| Subject) + (0 + Ve3| Subject), data = .)
AIC(resIntercept)


```
I used AIC to determine whether the model could be improved. Starting with the full model (absent the intercept), I attempted to remove each of the variables/effects in turn. Results of each AIC score came back larger than `r AIC(res)`, the AIC value for the model fit in b). The only model that improved upon AIC values was the one in which the intercept is added to the model. If the misinterpretation is permitted for small values of Ve2 and Ve3 then adding the intercept term may be worthwhile. The minimal gain in AIC however, (full = `r AIC(res)`, intercept = `r AIC(resIntercept)`), suggests that the interpretability retained is likely more worthwhile than the marginal gain in AIC.

d) Does there appear to be a difference in WOB between the sexes
```{r}
library(emmeans)
hold <- emmeans(res, "Sex")
pair <- pairs(hold)
pair
```
An attempt at using the emmeans package for comparison results in a p-value suggesting that there is a difference in work of breathing between the sexes. 
Another way to verify this result is to note that the interaction between Ve2 and Females was found to be not significantly different from 0 while the interaction between Ve3 and Males was found to be significant at the 5% level (t value = -2.171). As the correlation between Ve2 and Ve3 is `r cor(wob$Ve3, wob$Ve2)`, concluding based on the interaction of one factor level against Ve2 and the other factor level against Ve3 is nearly synonymous with the comparison of both against either Ve2 or Ve3. 

