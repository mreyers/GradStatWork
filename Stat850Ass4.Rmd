---
title: "Stat850 Assignment 4"
author: "Matthew Reyers"
date: "November 8, 2018"
output: html_document
---

This report contains my code based submission for Assignment 4 of Stat 850. 

Question 1: Consider the following model that I used in assignment 1:
y = 10sin(4pi^0.9)exp(-1.5x) + 10 + epsilon, where epsilon follows a N(0, 4) distribution. I assume the 4 to be representative of the variance, meaning that the standard deviation is 2.

This question will fit a GP to the 11 observations as in the assignment 1. Use a Gaussian process of the form given in the assignment question.
```{r}
library(tidyverse)

# Step 0: set up as before
set.seed(1)
x <- seq(0, 1, by = 0.1)
e <- rnorm(n = 11, sd = 2)

y <- 10*sin(4*pi*x^0.9)*exp(-1.5*x) + 10 + e
```

a) Write a function or subroutine to compute the covariance between two observations.
```{r}
covCalcBasic <- function(x1, x2){
  # Calculate the covariance between two given points
  
  # Get the y values
  locx1 <- which(x == x1)
  y1 <- y[locx1]
  
  locx2 <- which(x == x2)
  y2 <- y[locx2]
  
  # Compute covariance between the y values
  xvec <- c(x1, x2)
  yvec <- c(y1, y2)
  xbar <- mean(xvec)
  ybar <- mean(yvec)
  num <- sum((xvec - xbar)*(yvec - ybar)) # / (n-1) but n = 2
  
  
  # Return results
  return(num)
}
covCalcBasic(0.4, 0.4)

covCalcGaussian <- function(x1, x2, theta){
  
  covPW <- exp(-theta * (x1 - x2) ^ 2) # 1 dimensional predictors, given theta
  return(covPW)
}
```

b) Write a function or subroutine to compute the covariance matrix for the vector of 11 observations
```{r setup, include=FALSE}
covMatrixCalcBasic <- function(fullX, fullY){
  # Takes the two full vectors and calculates the covariance between the them pointwise
  
  # Create the matrix
  nrow <- length(fullX)
  ncol <- length(fullY)
  if(nrow != ncol){
    print("Not covariance function eligible")
    return(NA)
  }
  
  covMatrix <- matrix(0, nrow = nrow, ncol = ncol)
  
  # Fill the matrix with values
  for(i in 1:nrow){
    for(j in 1:ncol){
      covMatrix[i, j] <- covCalcBasic(x[i], x[j])
    }
  }
  return(covMatrix)
}

myResult <- covMatrixCalcBasic(x, y)
myResult

covMatrixGaussian <- function(fullX, fullY, sigmaz, sigmae, theta){
  
  # Create the matrix
  nrow <- length(fullX)
  ncol <- length(fullY)
  if(nrow != ncol){
    print("Not covariance function eligible")
    return(NA)
  }
  covMatrix <- matrix(0, nrow, ncol)
  
  # Fill the matrix with values
  for(i in 1:nrow){
    for(j in 1:ncol){
      covMatrix[i, j] <- covCalcGaussian(x[i], x[j], theta)
    }
  }
  sysCov <- sigmaz^2 * covMatrix + sigmae^2 * diag(dim(covMatrix)[1]) # Covariance of system depends on noise and signal
  return(sysCov)  
}
covMatrixG <- covMatrixGaussian(x, y, 1, 1, theta = 2)

```

c) Compute the inverse of the covariance function
```{r}
# Note first that cov(z(xi), z(xj)) = sigma_z^2 * r(xi, xj) where r(xi, xj) is the squared exponential
# I am also allowed to use 1 as the value for the variance components 
# This simplifies cov(z(xi), z(xj)) = r(xi, xj) where r(xi, xj)
invCov <- solve(covMatrixG)

# Other theta test
covMatrixTest <- covMatrixGaussian(x, y, sigmaz = 1, sigmae = 1, theta = 2)

```
My first attempt to compute the inverse of the covariance matrix resulted in a singular problem and therefore non-invertibility. This occurred with theta = 1. After testing with a few other theta values, it seems that theta <= 1.5 (roughly, not fully verified) will generate non-invertability conditions. This is because the covariance function does not die off as quickly as necessary to avoid the collinearity amongst columns. Larger theta values seem to resolve this issue. 
Update: My original covariance calculator was just for the correlation component, not scaled by sigmaz or added with sigmae scaled identity. I now no longer experience the same problems.

d) Write a function or subroutine to compute the log of the Gaussian likelihood for a Gaussian process
```{r }
likeFunction <- function(mu, theta, sigmaz, sigmae ){
  # Function calculates likelihood value given parameter values
  
  # Covariance structure
  covMatrix <- covMatrixGaussian(x, y, sigmaz, sigmae, theta)
  n <- dim(covMatrix)[1]
  sysCov <- sigmaz^2 * covMatrix + sigmae^2 * diag(n)
  
  # Likelihood components
  detSigma <- det(covMatrix)
  invSigma <- solve(covMatrix)
  muvec <- rep(mu, n = length(mu))
  exponent <- t(y - muvec) %*% invSigma %*% (y - muvec)
  
  # Likelihood
  likelihood <- (1 / sqrt(2* pi* detSigma))^(n/2)* exp(exponent)
  
  return(likelihood)
}


nllFunction <- function(mu, sigmaz, sigmae, theta ){
  # Function calculates likelihood value given parameter values
  
  # Covariance structure
  covMatrix <- covMatrixGaussian(x, y, sigmaz, sigmae, theta)
  n <- dim(covMatrix)[1]
  sysCov <- sigmaz^2 * covMatrix + sigmae^2 * diag(n)
  
  # Log likelihood requirements
  detSigma <- det(covMatrix)
  invSigma <- solve(covMatrix)
  muvec <- rep(mu, n = length(mu))
  exponent <- t(y - mu) %*% invSigma %*% (y - mu)

  # Likelihood and negative log likelihood
  likelihood <- (1 / sqrt(2* pi* detSigma))^(n/2)* exp(exponent)
  nll <- -1 * log(likelihood)
  
  return(nll)
}
nllFunction(mu = 1, theta = .55, sigmaz = 1, sigmae = 1)
```

e) Maximize the log likelihood function
We know the maximum likelihood of a Gaussian process for mu is the mean of the sample. We can then maximize the likelihood over a vector of thetas.
```{r}
vecTheta <- seq(1.5, 10, by = 0.1)
library(stats4)
mleResults <- mle(nllFunction, start = list(mu = mean(y), theta = 2, sigmaz = 1, sigmae = 1), method = "CG")
testoptim <- optim(list(mu = mean(y), theta = 2, sigmaz = 1, sigmae = 1), nllFunction, method = "Nelder-Mead", hessian = FALSE)
test <- optim(par = c(0, 0, 0, 0, 0), nllFunction, method = "Nelder-Mead", hessian = FALSE)
# Adjustment suggestions found here: https://stackoverflow.com/questions/28185387/non-finite-finite-difference-value-many-data-become-inf-and-na-after-exponentia
```

f) Using a random sample 50 inputs between 0 and 1, predict the output at the 50 different inputs

g) Compute (i) the maximum error over your 50 different inputs and (ii) the mean squared prediction error over your 50 different inputs

h) How does this compare to the polynomial regression from assignment 1?

2. A study was conducted to see if the height of a nest would impact the weights of baby birds. Nests were placed at different heights and the weights of baby birds one week after hatching were measured. The data are in the code below
```{r}
# Bird work
Nest <- c(1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 5)
Height <- c(10, 10, 10, 10, 15, 15, 15, 15, 20, 20, 20)
Weight <- c(7.5, 8.0, 7.1, 6.7, 9.1, 8.6, 9.6, 9.2, 9.9, 9.8, 10.2)
birds <- data.frame(Nest, Height, Weight)
```
a) What model would you fit to these data?
This situation involves an outcome (weight) and factors believed to possibly impact the outcome. The Nest factor represents a random factor as it is a variable we did not control for nor have a research question pertaining to. The Height factor is instead a fixed effect factor as we specifically changed the height of the nests and are looking to draw conclusions about these specific heights. The setup is suggestive of a Mixed Effects Model and is what I will implement to better understand the data.

b) Fit the model and report your conclusions with respect to the scientific question of interest
```{r}
# Plot the data
birds %>% ggplot(aes(factor(Height), Weight)) + 
  geom_point() +
  ggtitle("Plot of bird weight against nest height") +
  theme_minimal()

```
Initial data plotting does not seem to indicate any excessive outliers or cause for concern about equal variance among groups. With no assumptions horribly violated, I will proceed with fitting my Mixed Effect Model.
```{r}
library(lme4)
birds <- birds %>% mutate(Height = factor(Height),
                 Nest = factor(Nest))
linMixModel <- lmer(Weight ~ Height + (1 | Nest), data = birds, REML = FALSE)
summary(linMixModel)
plot(fitted(linMixModel), residuals(linMixModel), xlab = "Fitted values", ylab = "Residuals")

# Slightly different Mixed model
linRandSlope <- lmer(Weight ~ Height + (1 + Height | Nest), data = birds, REML = FALSE)
summary(linRandSlope)

# Compare the mixed effects
anova(linMixModel, linRandSlope)

# Out of curiosity, compare Mixed effects model to linear regression
standardLM <- lm(Weight ~ Height + Nest, data = birds)
summary(standardLM)
BIC(standardLM)
AIC(standardLM)
```
The Mixed Effect Model fit above suggests that the Fixed Effect Height and a non-zero intercept term constitute a useful model for understanding the weight of young birds. I would conclude that the Height of the nest has an effect on the weight of birds during development. Further, the fit indicated a small amount of the variation in Weight is attributable to the Random Effect Nest. This amount is nearly identical to the amount explained by noise and suggests that the random effect may not be necessary for the model.

This version of the Mixed Effect Model can be refered to as a Random Intercept Model. Out of curiosity, I compared the results of the Random Intercept Model with those of a Random Slope Model and a basic linear regression. With respect to the Random Slope Model, the given anova comparison indicates that the two models are insignificantly different from each other in terms of fit. This suggests that we take the more parsimonious model to be our choice, this being the Random Intercept Model.

Comparison with the linear regression equivalent (done by treating Nest as a regressor but not a random effect) yields a model that possesses similar AIC and BIC scores. The anova function is unable to compare across model types, such as Mixed Effect vs. Linear Regression, so our comparison toolkit is somewhat limited. As such I compared AIC and BIC scores of each model, finding the scores to be most similar to that of the Random Slope model while also bearing resemblance to the Random Intercept Model. Though the Random Intercept Model possesses slightly lower AIC and BIC values, the tendency to take the simpler model would suggest a linear regression model to be sufficient in this case.

3. 
