---
title: "Stat850 Assignment 4"
author: "Matthew Reyers"
date: "November 8, 2018"
output: html_document
---

This report contains my code based submission for Assignment 4 of Stat 850. 

Question 1: Consider the following model that I used in assignment 1:
y = 10sin(4pi^0.9)exp(-1.5x) + 10 + epsilon, where epsilon follows a N(0, 4) distribution. I assume the 4 to be representative of the variance, meaning that the standard deviation is 2.

This question will fit a GP to the 11 observations as in the assignment 1. Use a Gaussian process of the form given in the assignment question.
```{r}
library(tidyverse)

# Step 0: set up as before
set.seed(1234)
x <- seq(0, 1, by = 0.1)
e <- rnorm(n = 11, sd = 2)

y <- 10*sin(4*pi*x^0.9)*exp(-1.5*x) + 10 + e
#yact <- 10*sin(4*pi*x^0.9)*exp(-1.5*x) + 10
```

a) Write a function or subroutine to compute the covariance between two observations.
```{r}

covCalcGaussian <- function(x1, x2, theta){
  
  covPW <- exp(-theta * (x1 - x2) ^ 2) # 1 dimensional predictors, given theta
  return(covPW)
}
```

b) Write a function or subroutine to compute the covariance matrix for the vector of 11 observations
```{r setup, include=FALSE}

covMatrixR <- function(X1, X2, theta){
  
  # Create the matrix
  nrow <- length(X1)
  ncol <- length(X2)
  if(nrow != ncol){
    print("Not covariance function eligible")
    return(NA)
  }
  covMatrix <- matrix(0, nrow, ncol)
  
  # Fill the matrix with values
  for(i in 1:nrow){
    for(j in 1:ncol){
      covMatrix[i, j] <- covCalcGaussian(X1[i], X2[j], theta)
    }
  }
  return(covMatrix)
}

covMatrixGaussian <- function(sigmaz, sigmae, covMatrix){
  sysCov <- (sigmaz)^2 * covMatrix + (sigmae)^2 * diag(dim(covMatrix)[1]) # Covariance of system depends on noise and signal
  return(sysCov)  
}

covMatrixtemp <- covMatrixR(x, x, 5)
covMatrixG <- covMatrixGaussian(1, 1, covMatrixtemp)

```

c) Compute the inverse of the covariance function
```{r}
# Note first that cov(z(xi), z(xj)) = sigma_z^2 * r(xi, xj) where r(xi, xj) is the squared exponential
# I am also allowed to use 1 as the value for the variance components 
# This simplifies cov(z(xi), z(xj)) = r(xi, xj) where r(xi, xj)
invCov <- solve(covMatrixG)

# Other theta test
covMatrixTest <- covMatrixGaussian(x, x, sigmaz = 1, sigmae = 1, theta = 2)

```
My first attempt to compute the inverse of the covariance matrix resulted in a singular problem and therefore non-invertibility. This occurred with theta = 1. After testing with a few other theta values, it seems that theta <= 1.5 (roughly, not fully verified) will generate non-invertability conditions. This is because the covariance function does not die off as quickly as necessary to avoid the collinearity amongst columns. Larger theta values seem to resolve this issue. 
Update: My original covariance calculator was just for the correlation component, not scaled by sigmaz or added with sigmae scaled identity. I now no longer experience the same problems.

d) Write a function or subroutine to compute the log of the Gaussian likelihood for a Gaussian process
```{r }
nllFunction <- function(mu, sigmaz, sigmae, theta ){
  # Function calculates likelihood value given parameter values
  
  # Covariance structure
  
covMatrixtemp <- covMatrixR(x, x, theta)
covMatrixG <- covMatrixGaussian(sigmaz, sigmae, covMatrixtemp)
  
  #n <- dim(covMatrix)[1]
  #sysCov <- sigmaz^2 * covMatrix + sigmae^2 * diag(n)
  
  # Log likelihood requirements
  #detSigma <- det(covMatrixG) 
  invSigma <- solve(covMatrixG) 
  muvec <- rep(mu, n = length(mu))
  exponent <- -1*t(y - muvec) %*% invSigma %*% (y - muvec) / 2

  # Likelihood and negative log likelihood
  loglike <- -1*log(det(2*pi*covMatrixG)) / 2 + exponent
  nll <- -1*loglike
  
  return(nll)
}
nllFunction(mu = mean(y), theta = 5, sigmaz = 2, sigmae = 2)


```

e) Maximize the log likelihood function
We expect the mle for the mean of a Gaussian to be the mean of the sample and as such we will initialize the MLE conditions with mu = mean(y). I will then set theta to some initial value and allow it to vary. The following work is done with respect to sigmaz and sigmae being held fixed at 1 as per the example in the assignment. 
```{r}
library(stats4)
# MLE calculation is for all 4 of the parameters
mleResults <- mle(nllFunction, start = list(mu = mean(y), theta = 10, sigmaz = 1, sigmae = 1))
estimates <- mleResults@coef
# This mle seems to just accept theta at its given value (roughly)
# What I notice is that this pulls the muhat estimate away from its actual value, based on knowledge of Gaussian likelihood
# Iterate through theta start values until first muhat is equal to mean(y)
holdermu <- c()
for(i in 1:99){
  mui <- stats4::mle(nllFunction, start = list(mu = 10, theta = i, sigmaz = 1, sigmae = 1))
  mui2 <- mui@coef[1]
  holdermu <- rbind(holdermu, mui2)
}
holdermu

specificEst <- stats4::mle(nllFunction, start = list(mu = 10, theta = 102, sigmaz = 1, sigmae = 1))
#mleResultsNoFixed <- stats4::mle(nllFunction, start = list(mu = mean(y), theta = 2, sigmaz = 1, sigmae = 1))

nllFunction(mu = mleResults@coef[1], theta = mleResults@coef[4], sigmaz = 1, sigmae = 1)
nllFunction(mu = mleResults@coef[1], theta = mleResults@coef[4], sigmaz = mleResults@coef[2], sigmae = mleResults@coef[3])



```
```{r}
# ################## #
# Deprecated #
# ################## #

# Non- mle function approach
muvec <- seq(5, 15, by =0.1)
thetaVec <- seq(1, 200, by = 10)
resultsStore <- matrix(0, nrow = length(muvec), ncol = length(thetaVec))


fixedNLL <- function( theta, mu ){
  # Function calculates likelihood value given parameter values
  
  # Covariance structure
  covMatrix <- covMatrixGaussian(x, y, 1, 1, theta)
  n <- dim(covMatrix)[1]
  sysCov <- 1^2 * covMatrix + 1^2 * diag(n)
  
  # Log likelihood requirements
  detSigma <- det(covMatrix)
  invSigma <- solve(covMatrix)
  muvec <- rep(mu, n = length(mu))
  exponent <- -1*t(y - mu) %*% invSigma %*% (y - mu) / 2

  # Likelihood and negative log likelihood
  loglike <- log(1 / sqrt(2* pi* detSigma)) + exponent
  nll <- -1 * loglike
  
  return(nll)
}

theta1=thetaVec  
mu1=muvec  
post=outer(theta1,mu1,Vectorize(fixedNLL))    
image(theta1,mu1,post,xlab="theta",ylab="mu") 
contour(theta1,mu1,post,add=T) 


# Adjustment suggestions found here: https://stackoverflow.com/questions/28185387/non-finite-finite-difference-value-many-data-become-inf-and-na-after-exponentia
# ############### #
# Deprecated #
# ############### #
```


f) Using a random sample 50 inputs between 0 and 1, predict the output at the 50 different inputs
```{r}
set.seed(2)
xstar <- runif(50)

# Write new covariance matrix setup as prediction requires multiplication by sigmaz after choleski
predCovPointwise <- function(xstar_1, fullX, thetaHat){
  covVector <- exp(-thetaHat * (xstar_1 - fullX)^2)
}

predCovMatrix <- function(xstar_vec, fullX, thetaHat){
  
  nrow <- length(xstar_vec)
  ncol <- length(fullX)
  covMatrix <- matrix(0, nrow, ncol)
  
  # Fill the matrix with values
  for(i in 1:nrow){
    covMatrix[i, ] <- predCovPointwise(xstar_vec[i], fullX, thetaHat)
  }
  return(covMatrix)
}

mleEst <- mleResults@coef
covMatrixPred <- predCovMatrix(xstar, x, mleEst[4])
muhatVec <- rep(mleEst[1], n = length(x))

sigmaHatTemp <- covMatrixR(x, x, mleEst[4])
# Screwing around
sigmaHat <- covMatrixGaussian(mleEst[2], mleEst[3], sigmaHatTemp)

# Basic prediction
x1star <- xstar[1]
yhatx1star <- mleEst[1] + mleEst[2]^2 * t(predCovPointwise(x1star, x, mleEst[4]))  %*% solve(sigmaHat) %*% (y - muhatVec)

# All yhats, iterate
yhat <- c()
for(i in 1:length(xstar)){
  yhatTemp <- mleEst[1] + mleEst[2]^2 * t(predCovPointwise(xstar[i], x, mleEst[4]))  %*% solve(sigmaHat) %*% (y - muhatVec)
  yhat <- rbind(yhat, yhatTemp)
}

actualData <- data.frame(x, y)
predData <- data.frame(xstar, yhat)
ggplot(data = predData, aes(x = xstar, y = yhat)) + geom_point() + ggtitle("Plot of what I think are predicted values") + geom_point(data = actualData, aes(x = x, y = y, col = "red")) 

# Currently fits a flat line
############
# Investigate what happens if I cap sigma_e as some percentage ofthe variance, similar to Derek's suggestion
# E.g sigma_e ^ 2 <= 0.3 * var(y)







R <- predCovMatrix(xstar, mleResults@coef[[2]]) # Calculate covariance matrix at MLE, R
L <- chol(R + diag(10^(-10), dim(R)[1])) # Take the Choleski decomposition
L <- t(L) # Set L to be L transpose
w <- as.matrix(rnorm(50)) # Generate the same number of random normals as we have predictions of interest
sigma_z <- 1 # Using sigma_z fixed at 1
z <- sigma_z * L %*% w
newEpsilon <- rnorm(50)
muhat <- mleResults@coef[[1]]
muhatVec <- rep(muhat, n = 50)
yNew <- rep(muhat, n = 50) + z + newEpsilon 

# Now fitting the prediction model specified in notework
yhat <- muhatVec + sigma_z^2 * R %*% solve(sigma_z^2 * R + 1 * diag(dim(R)[1])) %*% (yNew - muhatVec) 



# Prediction is now at muhat + z
preds <- mleResults@coef[[1]] + z
predData <- data.frame(xstar, preds)
predData2<- data.frame(xstar, yhat)

predData2 %>% ggplot(aes(xstar, yhat)) + geom_point() 
#+ggtitle("Plot of predictions along GP using MLE results") + geom_point(data = moreData, aes(x = xAll, y = yFull, shape = "*", col = "red")) + geom_point(data = actualData, aes(x, yact, col = "blue"))


# Actual, for comparison
actualData <- data.frame(x, yact)
xAll <- seq(0, 1, 0.01)
yFull <- 10*sin(4*pi*xAll^0.9)*exp(-1.5*xAll) + 10
moreData <- data.frame(xAll, yFull)


# Plot shows it is not currently working
# Need to get the process to filter through the fixed points. Something a little funky
predData %>% ggplot(aes(xstar, preds)) + geom_point()
#+ggtitle("Plot of predictions along GP using MLE results") + geom_point(data = moreData, aes(x = xAll, y = yFull, shape = "*", col = "red")) + geom_point(data = actualData, aes(x, yact, col = "blue"))
```


g) Compute (i) the maximum error over your 50 different inputs and (ii) the mean squared prediction error over your 50 different inputs

h) How does this compare to the polynomial regression from assignment 1?

2. A study was conducted to see if the height of a nest would impact the weights of baby birds. Nests were placed at different heights and the weights of baby birds one week after hatching were measured. The data are in the code below
```{r}
# Bird work
Nest <- c(1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 5)
Height <- c(10, 10, 10, 10, 15, 15, 15, 15, 20, 20, 20)
Weight <- c(7.5, 8.0, 7.1, 6.7, 9.1, 8.6, 9.6, 9.2, 9.9, 9.8, 10.2)
birds <- data.frame(Nest, Height, Weight)
```
a) What model would you fit to these data?
This situation involves an outcome (weight) and factors believed to possibly impact the outcome. The Nest factor represents a random factor as it is a variable we did not control for nor have a research question pertaining to. The Height factor is instead a fixed effect factor as we specifically changed the height of the nests and are looking to draw conclusions about these specific heights. The setup is suggestive of a Mixed Effects Model and is what I will implement to better understand the data.

b) Fit the model and report your conclusions with respect to the scientific question of interest
```{r}
# Plot the data
birds %>% ggplot(aes(factor(Height), Weight)) + 
  geom_point() +
  ggtitle("Plot of bird weight against nest height") +
  theme_minimal()

```
Initial data plotting does not seem to indicate any excessive outliers or cause for concern about equal variance among groups. With no assumptions horribly violated, I will proceed with fitting my Mixed Effect Model.
```{r}
library(lme4)
birds <- birds %>% mutate(Height = factor(Height),
                 Nest = factor(Nest))
linMixModel <- lmer(Weight ~ Height + (1 | Nest), data = birds, REML = FALSE)
summary(linMixModel)
plot(fitted(linMixModel), residuals(linMixModel), xlab = "Fitted values", ylab = "Residuals")

# Slightly different Mixed model
linRandSlope <- lmer(Weight ~ Height + (1 + Height | Nest), data = birds, REML = FALSE)
summary(linRandSlope)

# Compare the mixed effects
anova(linMixModel, linRandSlope)

# Out of curiosity, compare Mixed effects model to linear regression
standardLM <- lm(Weight ~ Height + Nest, data = birds)
summary(standardLM)
BIC(standardLM)
AIC(standardLM)
```
The Mixed Effect Model fit above suggests that the Fixed Effect Height and a non-zero intercept term constitute a useful model for understanding the weight of young birds. I would conclude that the Height of the nest has an effect on the weight of birds during development. Further, the fit indicated a small amount of the variation in Weight is attributable to the Random Effect Nest. This amount is nearly identical to the amount explained by noise and suggests that the random effect may not be necessary for the model.

This version of the Mixed Effect Model can be refered to as a Random Intercept Model. Out of curiosity, I compared the results of the Random Intercept Model with those of a Random Slope Model and a basic linear regression. With respect to the Random Slope Model, the given anova comparison indicates that the two models are insignificantly different from each other in terms of fit. This suggests that we take the more parsimonious model to be our choice, this being the Random Intercept Model.

Comparison with the linear regression equivalent (done by treating Nest as a regressor but not a random effect) yields a model that possesses similar AIC and BIC scores. The anova function is unable to compare across model types, such as Mixed Effect vs. Linear Regression, so our comparison toolkit is somewhat limited. As such I compared AIC and BIC scores of each model, finding the scores to be most similar to that of the Random Slope model while also bearing resemblance to the Random Intercept Model. Though the Random Intercept Model possesses slightly lower AIC and BIC values, the tendency to take the simpler model would suggest a linear regression model to be sufficient in this case.

3. Ventilation in tests to exhaustion. Model is roughly W = aV^2 + bV^3 + epsilon. Differences exist on an individual level as a and b are population parameters. May also be effect of sex, impacting slope of each.

a) Propose a model relating work of breathing to ventilation that accounts for the multiple 
measurements per person and also sex differences.

It was suggested that each person differs from the population results by some quantity. Since we have multiple observations for each person, it would be of use for us to calculate the their individual values according to some model. I would suggest identifying an individual's specific a and b values according to a regression equation fit to their observations specifically. This would then lead to a population level random effects model in which W = (a + alpha_i)V^2 + (b + beta_i)V^3 + epsilon. A consideration to make is that each person's individual deviation from the population may be the cause of multiple differences. Some of these differences are expressable in fixed effects (sex) and random effects (individual performance differences). Model design should aim to incorporate both.
