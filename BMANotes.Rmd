---
title: "Bayesian Model Averaging"
author: "Matthew Reyers, Dani Chu"
date: "November 12, 2018"
output: html_document
---

Bayesian Model Averaging Notes with Dani, Presentation for the 16th of November and eventual write up
Currently reviewing: Bayesian model averaging: A systematic review and conceptual classification 
URL: https://arxiv.org/pdf/1509.08864.pdf

Why average models?
- Multiple models commonly do a good job of describing data generating events
- Choosing best of good models often like splitting hairs
- Single model selection leads to potential for overconfident inference, riskier decisions
  - Why? Ignores model uncertainty and makes particular distribution assumptions


Why use Bayesian approaches?
- Natural extension to traditional Bayesian Inference framework
- Captures uncertainty in posterior parameter and model posteriors

What is Bayesian Model Averaging?
- For each modeling approach, calculate the posterior distribution generated by each model according to Bayes theorem
  - Results in posterior(param estimate in model l) = Likelihood()*prior() for model l / product(likelihood()*prior(), l = 1,..., K)
  - The prior for this posterior is then decided via the individual model's capability to explain the data

Cool connections
- Bayes factors can be used to compare model l against model m in likelihood ratio manner
  - Compares posterior(Ml|Y)/poster(Mk|Y)
  - Can in turn rewrite the posterior probability of a given model Ml in terms of Bayes Factors
 
 
Challenges with BMA
- Prior specification is non-trivial
- Calculation of model evidence hard in non-conjugate problems

Uses of BMA
- Model combination for
  - Prediction
  - Model selection (covariate choices)
- Prediction of a new point involves averaging the predictions of all models we have posteriors for
- Parameter estimation for a parameter common to all of our models involves weighting the model outputs for that parameter
